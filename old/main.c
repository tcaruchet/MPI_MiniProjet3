#define N_DIM 4

#include <stdio.h>
#include <stdlib.h>
#include <omp.h>
#include <math.h>
#include <time.h>
#include <string.h>
#include <mpi.h>



int main(int argc, char const *argv[])
{
    /* code */
    //It is simply to implement the multiplication of a matrix of size NxM by a vector of size M, in order to output a new vector of size M. The product itself that each of the P processes in the ring will run, may be parallelized using OpenMP. The matrix of size  N x M will be represented as a struct. To simplify, you can assume that M=N.
    //A matrix can be represented as a 2D array, or be linearized. In that case, the more easy is to code some methods like get(x,y) and set(x,y,value) to access to the M[x,y] cell, delegating to these methods the duty to correctly deduce the address of that cell in the linearized representation. 


    // Assume that the process 0 is the one having access (in the file system), to the whole matrix, and to the vector, and that will collect back individual values of the resulting vector of size N. The topology to share information must be the unidirectional ring. You cannot use any provided MPI primitive that is capable to broadcast, scatter or gather data inside the communicator. So, you have to use the primitives that were developed in the previous exercices. Regarding the broadcast primitive, that you need to use to spread the vector to all processes before they can multiply it by the provided lines of the matrix, you can simply implement the naive version that is not pipelined.

    // To simplify, you assume that the total number of processes in the ring are P, and N is divisible by P. You can start by coding a version where P=N. So, each process will have to handle just one line of the matrix, multiply it by the formerly broadcast vector, getting back a single integer, all these integers being gathered on the process 0, to form the final vector.
    // Then, you can modify this version, in order to handle N/P lines of the matrix, N/P integers as result of the multiplication on each process, and gathering arrays of size N/P on the process 0, in order to get the final ouput vector of size N. The key point is that the whole matrix will be scattered by blocks of N/P lines, so that each of the P processes in order will be responsible of multiplying one such block by the broadcast vector.
    // 
    // Below you find a sequential program that will work on tiny examples of a vector, an input matrix (both generated by the program itself), and produces an output vector.
    // 
    // You also find below a piece of MPI C code in order to read a square matrix from a file name passed as parameter, whose content are lines, with as many integers as columns or lines. Be careful that each integer is separated from the next by a space, and the last integer of a line also is followed by a space. Up to you to adapt it to also read a vector from a second file name passed as parameter.

    // Sample command : mpirun -np xx matmult <file with integers representing the matrix> <file with integers representing the vector>

    //variables string
    char *file_matrix_name = argv[5];
    char *file_vector_name = argv[6];

    //Valid Communicator Size?
    if (N_DIM%size){  
        MPI_Finalize();
        return(0);
    }

    //Initialize MPI
    int rank, size;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    //Initialize variables
    int i,j,k,l;
    int n = N_DIM;
    int p = size;
    int n_lines = n/p;
    
    double matrix_data[N_DIM][N_DIM];  //matrix
    double vector_data[N_DIM];         //vector
    double result[N_DIM] = {0.0};   //final result holder

    if (rank==0){
        read_mat_from_file(file_matrix_name, N_DIM, N_DIM, (double *)matrix_data);   //Populating the Matrix
        read_mat_from_file(file_vector_name, N_DIM, 1, vector_data);                 //Populating the Vector
    }

   RowMatrixVectorMultiply(N_DIM, (double *)matrix_data, vector_data,result);
    
    /* Printing the Matrix*/
    if (rank==0){
        printf("Matrix  :\n");
        for (int i=0;i<N_DIM;i++){
            for (int j=0;j<N_DIM;j++)
                printf("%.5f ", matrix_data[i][j]);
            printf("\n");
        }
        printf("Vector :\n");
        for (int i=0;i<N_DIM;i++)
            printf("%.5f ", vector_data[i]);
        printf("\n\n");
        
        printf("Vector :\n");
        for (int i=0;i<N_DIM;i++)
            printf("%.5f ", vector_data[i]);
        printf("\n\n");
        
        printf("Result :\n");
        for (int i=0;i<N_DIM;i++)
            printf("%.5f ", result[i]);
        printf("\n\n");
    }
    
    //Print the Results
    if (rank==0){
        printf("\n");
        for (i=0; i<N_DIM; i++){
            printf("%f\n", result_gather[i]);
        }
    }

    //Finalize MPI
    MPI_Finalize();

    return 0;
}


void read_mat_from_file(char *filename, int n_rows, int n_cols, double *matrix_data){
    FILE *fp;
    fp = fopen(filename, "r");
    if (fp == NULL){
        printf("Error opening file!\n");
        exit(1);
    }
    int i,j;
    for (i=0; i<n_rows; i++){
        for (j=0; j<n_cols; j++){
            fscanf(fp, "%lf", &matrix_data[i*n_cols+j]);
        }
    }
    fclose(fp);
}  

void RowMatrixVectorMultiply(int n, double *matrix_data, double *vector_data, double *result){
    int rank,size;
    MPI_Status status;
    MPI_Comm_rank (MPI_COMM_WORLD, &rank);
    MPI_Comm_size (MPI_COMM_WORLD, &size); 
    double* localresult = new double[dim / size]{};  //local result
    double matrix [dim][dim];   //local matrix
    double timer=MPI_Wtime();
    int i,j,k;

    //Comm Start//////
    //MPI_Scatter(matrix_data, (dim*dim)/size, MPI_DOUBLE, matrix, (dim*dim)/size, MPI_DOUBLE, 0, MPI_COMM_WORLD); //Scatter the Matrix
    if (rank==0){
        for (int i=0;i<dim/size;i++)
            for (int j=0;j<dim;j++)
                matrix[i][j] = matrix_data[j+(dim*i)];
        for (int i=1;i<size;i++){    
            MPI_Send(matrix_data+((dim*dim/size)*(i)),(dim*dim)/size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
        }
    }
    else{
        MPI_Recv(matrix, dim*dim/size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD,MPI_STATUS_IGNORE);
    }

    ////Comm End-Start
    //Broadcast
    if (rank == 0) {
        for (int i = 0; i < size; i++) {
            if (i != rank) {
                MPI_Send(vector_data, dim, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
            }
        }
    } else {
        MPI_Recv(vector_data, dim, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD,MPI_STATUS_IGNORE);
    }
    //Comm End//////



    //Matrix Multiply Start//////
    for (i=0; i<dim/size; i++){
        for (j=0; j<dim; j++){
            for (k=0; k<dim; k++){
                localresult[i] += matrix[i][k]*vector_data[k];
            }
        }
    }
    //Matrix Multiply End//////

    //Gather Start//////
    if (rank==0){
        for (int i=0;i<dim/size;i++)
            result[i] = localresult[i];
        for (int i=1;i<size;i++){    
            MPI_Recv(result+((dim/size)*(i)),(dim)/size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD,MPI_STATUS_IGNORE);
        }
    }
    else
        MPI_Send(localresult, dim/size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);

    //Gather End//////
    timer = MPI_Wtime()-timer;
    if (rank==0)
        cout << "Time Needed for all ops = "<<timer<<endl;

    return;
}